{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Data compression fundamentals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Why to compress?\n",
    "\n",
    "* Data compression can reduce the memory requirements of (almost) any kind of\n",
    "  information source."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Which data?\n",
    "  \n",
    "* Mainly ... audio, image, and video signals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Why these sources?\n",
    "\n",
    "* After the digitalization of any signal we get a sequence $s[]$\n",
    "  of samples that represent the signal $s$ with more or less fidelity.\n",
    "  \n",
    "* Usually, $s[]$ is encoded using PCM (Pulse Code Modulation), in which\n",
    "  every sample $s[i]$ is represented with the same number of bits.\n",
    "  \n",
    "* Most digital PCM signals are memory demanding. For\n",
    "  example, in a CD we have a data-rate of\n",
    "  \n",
    "\\begin{equation}\n",
    "    (16+16)\\frac{\\text{bits}}{\\text{sample}}\\times\n",
    "    44{.}100\\frac{\\text{samples}}{\\text{second}}=\n",
    "    1{.}411{.}200\\frac{\\text{bits}}{\\text{second}}.\n",
    "\\end{equation}\n",
    "  \n",
    "* Image and video signals require much more memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Redundancy in signals\n",
    "\n",
    "In general, signals has three types of redundancy:\n",
    "    \n",
    "   1. **Spatial/temporal redundancy**: Produced by similarities between\n",
    "    adjacent (in 2D and 3D) samples. It can be removed using\n",
    "    spatial/temporal models of the signal, generating [*lossless\n",
    "    codecs*](https://en.wikipedia.org/wiki/Lossless_compression). These codecs are known as *audio, *image*, and *video* codecs.\n",
    "    \n",
    "   2. **Statistical redundancy**: Spatial/temporal redundancy generates *probabilistic relationships* among samples. Statistical redundancy can be removed by using\n",
    "    probabilistic models, producing also *lossless codecs*. These\n",
    "    codecs are known as *text codecs* because can be used to compress text sources.\n",
    "    \n",
    "   3. **Psychological redundancy**: Part of the information that\n",
    "    signals carry can not be perceived by humans. [*Lossy codecs*](https://en.wikipedia.org/wiki/Lossy_compression) remove\n",
    "    this kind of pseudo-redundancy, basically, by means of quantization.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Symbols, runs, strings, code-words, and code-streams!\n",
    "\n",
    "* In the context of statistical coding, each sample $s[i]$ is\n",
    "  called a [*symbol*](https://en.wikipedia.org/wiki/Symbol).\n",
    "  \n",
    "* Depending on the type of statistical relationship among\n",
    "  symbols, we will also speak about [*strings*](https://en.wikipedia.org/wiki/String_(computer_science) when we process\n",
    "  more than one symbol, and about [*runs*](https://en.wikipedia.org/wiki/Sequence#Sequences_and_automata) when all the symbols are\n",
    "  the same in a string.\n",
    "  \n",
    "* In any case, the output of the encoder is a sequence of\n",
    "  [*code-words*](https://en.wikipedia.org/wiki/Universal_code_(data_compression) that all together generates a *code-stream*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Some interesting compression insights\n",
    "\n",
    "* Lossless compressors are [bijective functions](https://en.wikipedia.org/wiki/Bijection) which find a different output for each possible input. For this reason, text compressors are lossless by definition.\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/a/a5/Bijection.svg/200px-Bijection.svg.png\"  style=\"width: 200px;\" align=\"middle\"/>\n",
    "\n",
    "* Lossy compressors are [surjective functions](https://en.wikipedia.org/wiki/Surjective_function) and therefore, two or more inputs can produce the same output. There are lossless audio, image, and video compressors, but most of them are lossy (although some of them can be lossless if quantization is used).\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/6/6c/Surjection.svg/220px-Surjection.svg.png\"  style=\"width: 200px;\" align=\"center\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A. [Run-length encoding](https://en.wikipedia.org/wiki/Run-length_encoding)\n",
    "\n",
    "* RLE (Run Length Encoding) is a technique that removes the data\n",
    "  redundancy produced by the repetition of symbols. Example:\n",
    "  ```\n",
    "  aaaaa = 5a\n",
    "  ```\n",
    "* Depending on the length of the source alphabet and the maximal length of the run,\n",
    "  different versions of RLE codecs have been proposed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### A.1 $N$-ary run length encoding\n",
    "\n",
    "RLE for $N$-ary alphabets (alphabets of size $N$), where typically, $N=256$.\n",
    "\n",
    "### Encoder\n",
    "\n",
    "1. While there are symbols to encode:\n",
    "    1. Let $s$ the next symbol.\n",
    "    2. Read the next $n$ consecutive symbols equal to $s$.\n",
    "    3. Write the pair $ns$.\n",
    "\n",
    "### Example\n",
    "\n",
    "Runs:\n",
    "```\n",
    "aaaabbbbbaaaaaabbbbbbbcccccc\n",
    "```\n",
    "are encoded as:\n",
    "```\n",
    "4a5b6a7b6c\n",
    "```\n",
    "\n",
    "### Decoder\n",
    "\n",
    "1. While there are $ns$ pairs to decode:\n",
    "    1. Write $n$-times the symbol $s$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(4, 'a'), (5, 'b'), (6, 'a'), (7, 'b'), (6, 'c')]\n",
      "aaaabbbbbaaaaaabbbbbbbcccccc\n"
     ]
    }
   ],
   "source": [
    "# https://rosettacode.org/wiki/Run-length_encoding#Python\n",
    "# https://docs.python.org/3/library/itertools.html#itertools.groupby\n",
    "\n",
    "from itertools import groupby\n",
    "def N_RLE_encode(input_string):\n",
    "    return [(len(list(g)), k) for k,g in groupby(input_string)]\n",
    " \n",
    "def N_RLE_decode(lst):\n",
    "    return ''.join(c * n for n,c in lst)\n",
    "\n",
    "x = N_RLE_encode('aaaabbbbbaaaaaabbbbbbbcccccc')\n",
    "print(x)\n",
    "y = N_RLE_decode(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### A.2 Binary run length encoding\n",
    "\n",
    "* In binary RLE is not necessary to indicate the next symbol\n",
    "  (only the length) because when a run ends, only the other (possible) symbol will\n",
    "  start with the next run.\n",
    "  \n",
    "### Encoder\n",
    "\n",
    "1. Let $s\\leftarrow$ `0`.\n",
    "2. While there are bits to encode:\n",
    "    1. Read the next $n$ consecutive bits equal to $s$.\n",
    "    2. Write $n$.\n",
    "    3. $s\\leftarrow (s+1)~\\text{modulus}~2$.\n",
    "\n",
    "### Example\n",
    "\n",
    "Runs:\n",
    "```\n",
    "0000111110000001111111000000\n",
    "```\n",
    "are encoded as::\n",
    "```\n",
    "4 5 6 7 6\n",
    "```\n",
    "\n",
    "### Decoder\n",
    "\n",
    "1. Let $s\\leftarrow$ `0`.\n",
    "2. While there are items $n$ to decode:\n",
    "    1. Write $n$ bits equal to $s$.\n",
    "    2. $s\\leftarrow (s+1)~\\text{modulus}~2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### A.3 [MPN-5 run length encoding](https://en.wikipedia.org/wiki/Microcom_Networking_Protocol#MNP_5)\n",
    "\n",
    "* Created by [Microcom Inc.](https://en.wikipedia.org/wiki/Microcom_Networking_Protocol)\n",
    "for the [MNP (Microcom Networking Protocol) 5](https://scholar.google.es/scholar?hl=es&as_sdt=0%2C5&q=held+data+compression+techniques+applications&btnG=).\n",
    "\n",
    "### Codec\n",
    "\n",
    "  following examples:\n",
    "  \n",
    "```\n",
    "Input     Output\n",
    "--------- ---------\n",
    "ab        ab\n",
    "aab       aab\n",
    "aaab      aaa0b /* 3-symbols length run == <ESC> code-word */\n",
    "aaaab     aaa1b\n",
    "aaaaab    aaa2b\n",
    ":         :\n",
    "a^nb      aaa(n-3)b\n",
    "```\n",
    "\n",
    "### Example\n",
    "\n",
    "Runs:\n",
    "```\n",
    "aaaabbbbbaaaaaabbbbbbbccccccb\n",
    "```\n",
    "are encoded as:\n",
    "```\n",
    "aaa1bbb2aaa3bbb4ccc3b\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aaa1bbb2aaa3bbb4ccc3aaa0b\n",
      "aaaabbbbbaaaaaabbbbbbbccccccaaab\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TO-DO\n",
    "from itertools import groupby #I import the library to use the groupby function\n",
    "def MPN5_enconding(input_string):#I define the method as MPN5_encoding\n",
    "    code_stream = '' #The string that I will return when it is encoded\n",
    "    for k, g in groupby(input_string): #The string I want to encode is separated in different runs\n",
    "        length_eachRun = len(list(g)) #Length of run\n",
    "        if length_eachRun > 3: #If lenght of run is equal or greater than 3, I treat it\n",
    "            code_stream+= str(k*3) + str((length_eachRun - 3)) #Calculate this: aaa(N-3) being N the lenght of run\n",
    "        elif length_eachRun == 3: \n",
    "            code_stream+= k*length_eachRun + '0'\n",
    "        else: #If its length is less than 3, I leave it as it is, I do not treat it\n",
    "            code_stream+= k*length_eachRun\n",
    "        #print(k, code_stream)\n",
    "    return code_stream #I return the already coded string\n",
    "\n",
    "def MPN5_decoding(input_string): #I define the method as MPN5_decoding by passing it a code-stream encoded by parameter\n",
    "    original_code = '' #The decoded string that I will return\n",
    "    for i in range(len(input_string)): #I go through each code-stream symbol\n",
    "        #print(i, input_string[i])\n",
    "        if input_string[i].isdigit(): #If it is a number, I will take the symbol that is repeated and write it the n times it repeats\n",
    "            run = input_string[i-1]*int(input_string[i])\n",
    "            original_code+= run\n",
    "        else:\n",
    "            original_code+= input_string[i] #If it is not a number, I write the symbol\n",
    "    return original_code #I return the original code-stream (not coded)\n",
    "\n",
    "message_encoded = MPN5_enconding('aaaabbbbbaaaaaabbbbbbbccccccaaab')\n",
    "print(message_encoded)\n",
    "message_decoded = MPN5_decoding(message_encoded)\n",
    "print(message_decoded)\n",
    "\n",
    "'aaaabbbbbaaaaaabbbbbbbccccccaaab' == message_decoded\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A.4 [Burrows-Wheeler Transform (BWT)](https://scholar.google.es/scholar?hl=es&as_sdt=0%2C5&q=Burrows+M%2C+Wheeler+DJ%3A+A+Block+Sorting+Lossless+Data+Compression+Algorithm.&btnG=)\n",
    "\n",
    "* BWT is an algorithm that inputs a string and outputs:\n",
    "  \n",
    "  1. A different string with the same symbols but with longer runs, and therefore potentially more compressible.\n",
    "     The lengths of the runs are proportional to the correlation\n",
    "     among symbols and the length of the input.\n",
    "  2. An index.\n",
    "  \n",
    "* The inverse transform, using the output of the\n",
    "  forward transform, recover the original string.\n",
    "  \n",
    "* Used in [`bzip2`](https://en.wikipedia.org/wiki/Bzip2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Encoder\n",
    "\n",
    "Let $B$ the block-size in symbols:\n",
    "\n",
    "1. While the input is not exhausted:\n",
    "    1. Read $B$ symbols in `S`.\n",
    "    2. (`L`, `I`) = BWT(`S`).\n",
    "    3. Output `L` (the output block with longer runs) and `I` (an index of a symbol in `L`).\n",
    "\n",
    "### Decoder\n",
    "\n",
    "1. While input pairs (`L`, `I`):\n",
    "    1. `S` = iBWT(`L`, `I`).\n",
    "    2. Output `S`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Forward BWT\n",
    "\n",
    "1. Input `S`, the sequence of `B` symbols.\n",
    "    ```\n",
    "    S = \"abraca.\"\n",
    "    ```\n",
    "    \n",
    "2. Compute a matrix `M'` with all possible cyclic rotations of `S`.\n",
    "\n",
    "    ```\n",
    "    M' = [\"abraca.\",\n",
    "          \"braca.a\",\n",
    "          \"raca.ab\",\n",
    "          \"aca.abr\",\n",
    "          \"ca.abra\",\n",
    "          \"a.abrac\",\n",
    "          \".abraca\"]\n",
    "    ```\n",
    "\n",
    "3. Sort lexicographically `M'` go generate `M`.\n",
    "    ```\n",
    "    M = [\".abraca\",\n",
    "         \"a.abrac\",\n",
    "         \"abraca.\",\n",
    "         \"aca.abr\",\n",
    "         \"braca.a\",\n",
    "         \"ca.abra\",\n",
    "         \"raca.ab\"]\n",
    "    ```\n",
    "\n",
    "3. Let `I` the index of `S` in `M`.\n",
    "    ```\n",
    "    I = 2\n",
    "    ```\n",
    "\n",
    "4. Let `L` the column `B`-1 of `M`.\n",
    "    ```\n",
    "    L = \"ac-raab\"\n",
    "    ```\n",
    "\n",
    "2. Output `I` and `L`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Inverse BWT\n",
    "\n",
    "The backward transform regenerates the `I`-th row of `M`. Here is an example:\n",
    "\n",
    "1. Input `I` and `L`, the output of a BWT applied to a string `S` of length `B`.\n",
    "\n",
    "2. The first `F` and the last `L` column of `M` are available taking into consideration that `F=sorted(L)`.\n",
    "    ```\n",
    "    F23456L\n",
    "    .     a\n",
    "    a     c\n",
    "    a     .\n",
    "    a     r\n",
    "    b     a\n",
    "    c     a\n",
    "    r     b\n",
    "    ```\n",
    "    \n",
    "3. Notice that for a particular cymcl in `L`, the corresponding symbol in `F` follow it in `S`. Therefore, we have found all pairs of `S` by taking pairs of `LF`.\n",
    "    ```\n",
    "    a.\n",
    "    ca\n",
    "    .a\n",
    "    ra\n",
    "    ab\n",
    "    ac\n",
    "    br\n",
    "    ```\n",
    "Which sorted:\n",
    "    ```\n",
    "    .a\n",
    "    a.\n",
    "    ab\n",
    "    ac\n",
    "    br\n",
    "    ca\n",
    "    ra\n",
    "    ```\n",
    "form the first two columns of `M`.\n",
    "\n",
    "4. Repeat the process: get the rest of the columns (here only a few are shown):\n",
    "\n",
    "    ```\n",
    "    F23456L\n",
    "    .a    a\n",
    "    a.    c\n",
    "    ab    .\n",
    "    ac    r\n",
    "    br    a\n",
    "    ca    a\n",
    "    ra    b\n",
    "    ```\n",
    "\n",
    "Now, for a particular symbol in `L`, the corresponding pair in columns `F 2` follows it in `S`. So, we can find all triples of `S` by tacking triples of `L F 2`:\n",
    "    ```\n",
    "    a.a         .ab\n",
    "    ac.         a.a\n",
    "    .ab  sort   abr\n",
    "    rac ------> aca\n",
    "    abr         bra\n",
    "    aca         ca.\n",
    "    bra         rac\n",
    "    ```\n",
    "    \n",
    "So, we have the partial reconstruction of `M`:\n",
    "    ```\n",
    "    F23456L\n",
    "    .ab   a\n",
    "    a.a   c\n",
    "    abr   . <- I\n",
    "    aca   a\n",
    "    bra   a\n",
    "    ca.   a\n",
    "    rac   b\n",
    "    ```\n",
    "\n",
    "In an optimized implementation of BWT, only the row `I` is generated."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "<!--\n",
    "2. Let be $F=\\text{sorted}(L)$, the first column of the matrix $M$ computed by $\\text{BWT}(S)$. $L$ is the first column of a matrix\n",
    "<br/>\n",
    "<br/>\n",
    "$$M'=[[M[i,(j-1)\\%n]~\\text{for}~j~\\text{in}~\\text{range}(n)]~\\text{for}~i~\\text{in}~\\text{range}(n)],$$\n",
    "<br/>\n",
    "formed by rotating each row of $M$ one character to the right. By rows, $M'$ is sorted lexicographically starting with their second character.\n",
    "    ```\n",
    "    S = \"abraca\" L = \"caraab\" F = \"aaabcr\" M = [\"aabrac\", M' = [\"caabra\",\n",
    "                                                \"abraca\",       \"aabrac\",\n",
    "                                                \"acaabr\",       \"racaab\",\n",
    "                                                \"bracaa\",       \"abraca\",\n",
    "                                                \"caabra\",       \"acaabr\",\n",
    "                                                \"racaab\"]       \"bracaa\"]\n",
    "    ```\n",
    "If we consider only those rows in $M'$ that start with a character *ch* (`a` for example), they must appear in lexicographical order relative to one another, because they have been sorted lexicographically starting with their second characters, and their first characters are all the same. Therefore, for any given character *ch*, the rows in $M$ that begin with *ch* appear in the same order as the rows in $M'$ that begin with *ch*.\n",
    "\n",
    "3. Using $F$ and $L$, the first columns of $M$ and $M'$ respectively, we calculate a vector $T$ that indicates the correspondence between the rows of the two matrices, in the sense that\n",
    "<br/>\n",
    "<br/>\n",
    "$$\n",
    "M'[j] == M[T[j]],~\\text{for}~j~\\text{in}~\\text{range}(n),\n",
    "$$\n",
    "<br/>\n",
    "which in terms of $F$ and $L$ corresponds to\n",
    "<br/>\n",
    "<br/>\n",
    "$$\n",
    "L[j] == F[T[j]],~\\text{for}~j~\\text{in}~\\text{range}(n).\n",
    "$$\n",
    "    ```\n",
    "    T = [4, 0, 5, 1, 2, 3] # Rows of M' in M\n",
    "    ```\n",
    "\n",
    "4. Since each row in $M$ is a rotation of $S$, the character $L[i]$ cyclicly precedes the character $F[i]$ in $S$.\n",
    "    ```\n",
    "    L[0] = \"c\" precedes F[0] = \"a\" in S = \"abraca\"\n",
    "    ```\n",
    "   If we take $i=T[j]$, $L[T[j]]$ cyclicly precedes $F[T[j]]=L[j]$ in $S$. Sumarizing:\n",
    "   <br/>\n",
    "   <br/>\n",
    "   $$\n",
    "   L[T[j]]~\\text{cyclicly precedes}~L[j]~\\text{in}~S,~\\text{for}~j~\\text{in}~\\text{range}(n).\n",
    "   $$\n",
    "   <br/>\n",
    "   Remember, the index $I$ is defined such that row $I$ of $M$ is $S$. Thus, the last character of $S$ is $L[I]$.\n",
    "   ```\n",
    "   S[n-1] = \"a\" = L[1]\n",
    "   ```\n",
    "   We can use the vector T to give the predecessors of each character, by defining:\n",
    "   <br/>\n",
    "   <br/>\n",
    "   $$\n",
    "   S[n-1-i] = L[T^i[I]], ~\\text{for}~i~\\text{in}~\\text{range}(n)\n",
    "   $$\n",
    "   <br/>\n",
    "   where $T^0[x]=x$ and $T^{i+1}[x] = T[T^i[x]]$. This is equivalent to reconstruct the reverse of $S$,\n",
    "   traveling $L$ as defined by `Tx`:\n",
    "   ```\n",
    "   Tx = [I]\n",
    "   for i in range(1, n):\n",
    "        Tx.append(T[Tx[i-1]])\n",
    "   ```\n",
    "   This computes:\n",
    "   ```\n",
    "   Tx[0] = 1                            T  = [4, 0, 5, 1, 2, 3]\n",
    "   Tx[1] = T[Tx[0]] = T[1] = 0          Tx = [1, 0, 4, 2, 5, 3]\n",
    "   Tx[2] = T[Tx[1]] = T[0] = 4                0  1  2  3  4  5\n",
    "   Tx[3] = T[Tx[2]] = T[4] = 2\n",
    "   Tx[4] = T[Tx[3]] = T[2] = 5\n",
    "   Tx[5] = T[Tx[4]] = T[5] = 3\n",
    "   ```\n",
    "   Finally:\n",
    "   ```\n",
    "   Reversed S = L[1], L[0], L[4], L[2], L[5], L[3] = a, c, a, r, b, a\n",
    "                                                 S = a, b, r, a, c, a\n",
    "   ```\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENCODING\n",
      "Original string:\n",
      "S = abraca.\n",
      "\n",
      "Permutations matrix:\n",
      "abraca. 0\n",
      "braca.a 1\n",
      "raca.ab 2\n",
      "aca.abr 3\n",
      "ca.abra 4\n",
      "a.abrac 5\n",
      ".abraca 6\n",
      "\n",
      "Sorted matrix:\n",
      ".abraca 0\n",
      "a.abrac 1\n",
      "abraca. 2\n",
      "aca.abr 3\n",
      "braca.a 4\n",
      "ca.abra 5\n",
      "raca.ab 6\n",
      "\n",
      "M' = M rotated one character to the right\n",
      "a.abrac\n",
      "ca.abra\n",
      ".abraca\n",
      "raca.ab\n",
      "abraca.\n",
      "aca.abr\n",
      "braca.a\n",
      "\n",
      "I = 2, L = ac.raab\n",
      "\n",
      "DECODING\n",
      "T = Positions of rows of M' in M: [1, 5, 0, 6, 2, 3, 4]\n",
      "Tx = Positions in L of output symbols (reversed) = [2, 0, 1, 5, 3, 6, 4]\n",
      "S = abraca.\n",
      "\n",
      "ENCODING\n",
      "Original string:\n",
      "S = abababababababababababababababababababa\n",
      "\n",
      "Permutations matrix:\n",
      "abababababababababababababababababababa 0\n",
      "bababababababababababababababababababaa 1\n",
      "ababababababababababababababababababaab 2\n",
      "babababababababababababababababababaaba 3\n",
      "abababababababababababababababababaabab 4\n",
      "bababababababababababababababababaababa 5\n",
      "ababababababababababababababababaababab 6\n",
      "babababababababababababababababaabababa 7\n",
      "abababababababababababababababaabababab 8\n",
      "bababababababababababababababaababababa 9\n",
      "ababababababababababababababaababababab 10\n",
      "babababababababababababababaabababababa 11\n",
      "abababababababababababababaabababababab 12\n",
      "bababababababababababababaababababababa 13\n",
      "ababababababababababababaababababababab 14\n",
      "babababababababababababaabababababababa 15\n",
      "abababababababababababaabababababababab 16\n",
      "bababababababababababaababababababababa 17\n",
      "ababababababababababaababababababababab 18\n",
      "babababababababababaabababababababababa 19\n",
      "abababababababababaabababababababababab 20\n",
      "bababababababababaababababababababababa 21\n",
      "ababababababababaababababababababababab 22\n",
      "babababababababaabababababababababababa 23\n",
      "abababababababaabababababababababababab 24\n",
      "bababababababaababababababababababababa 25\n",
      "ababababababaababababababababababababab 26\n",
      "babababababaabababababababababababababa 27\n",
      "abababababaabababababababababababababab 28\n",
      "bababababaababababababababababababababa 29\n",
      "ababababaababababababababababababababab 30\n",
      "babababaabababababababababababababababa 31\n",
      "abababaabababababababababababababababab 32\n",
      "bababaababababababababababababababababa 33\n",
      "ababaababababababababababababababababab 34\n",
      "babaabababababababababababababababababa 35\n",
      "abaabababababababababababababababababab 36\n",
      "baababababababababababababababababababa 37\n",
      "aababababababababababababababababababab 38\n",
      "\n",
      "Sorted matrix:\n",
      "aababababababababababababababababababab 0\n",
      "abaabababababababababababababababababab 1\n",
      "ababaababababababababababababababababab 2\n",
      "abababaabababababababababababababababab 3\n",
      "ababababaababababababababababababababab 4\n",
      "abababababaabababababababababababababab 5\n",
      "ababababababaababababababababababababab 6\n",
      "abababababababaabababababababababababab 7\n",
      "ababababababababaababababababababababab 8\n",
      "abababababababababaabababababababababab 9\n",
      "ababababababababababaababababababababab 10\n",
      "abababababababababababaabababababababab 11\n",
      "ababababababababababababaababababababab 12\n",
      "abababababababababababababaabababababab 13\n",
      "ababababababababababababababaababababab 14\n",
      "abababababababababababababababaabababab 15\n",
      "ababababababababababababababababaababab 16\n",
      "abababababababababababababababababaabab 17\n",
      "ababababababababababababababababababaab 18\n",
      "abababababababababababababababababababa 19\n",
      "baababababababababababababababababababa 20\n",
      "babaabababababababababababababababababa 21\n",
      "bababaababababababababababababababababa 22\n",
      "babababaabababababababababababababababa 23\n",
      "bababababaababababababababababababababa 24\n",
      "babababababaabababababababababababababa 25\n",
      "bababababababaababababababababababababa 26\n",
      "babababababababaabababababababababababa 27\n",
      "bababababababababaababababababababababa 28\n",
      "babababababababababaabababababababababa 29\n",
      "bababababababababababaababababababababa 30\n",
      "babababababababababababaabababababababa 31\n",
      "bababababababababababababaababababababa 32\n",
      "babababababababababababababaabababababa 33\n",
      "bababababababababababababababaababababa 34\n",
      "babababababababababababababababaabababa 35\n",
      "bababababababababababababababababaababa 36\n",
      "babababababababababababababababababaaba 37\n",
      "bababababababababababababababababababaa 38\n",
      "\n",
      "M' = M rotated one character to the right\n",
      "baababababababababababababababababababa\n",
      "babaabababababababababababababababababa\n",
      "bababaababababababababababababababababa\n",
      "babababaabababababababababababababababa\n",
      "bababababaababababababababababababababa\n",
      "babababababaabababababababababababababa\n",
      "bababababababaababababababababababababa\n",
      "babababababababaabababababababababababa\n",
      "bababababababababaababababababababababa\n",
      "babababababababababaabababababababababa\n",
      "bababababababababababaababababababababa\n",
      "babababababababababababaabababababababa\n",
      "bababababababababababababaababababababa\n",
      "babababababababababababababaabababababa\n",
      "bababababababababababababababaababababa\n",
      "babababababababababababababababaabababa\n",
      "bababababababababababababababababaababa\n",
      "babababababababababababababababababaaba\n",
      "bababababababababababababababababababaa\n",
      "aababababababababababababababababababab\n",
      "abaabababababababababababababababababab\n",
      "ababaababababababababababababababababab\n",
      "abababaabababababababababababababababab\n",
      "ababababaababababababababababababababab\n",
      "abababababaabababababababababababababab\n",
      "ababababababaababababababababababababab\n",
      "abababababababaabababababababababababab\n",
      "ababababababababaababababababababababab\n",
      "abababababababababaabababababababababab\n",
      "ababababababababababaababababababababab\n",
      "abababababababababababaabababababababab\n",
      "ababababababababababababaababababababab\n",
      "abababababababababababababaabababababab\n",
      "ababababababababababababababaababababab\n",
      "abababababababababababababababaabababab\n",
      "ababababababababababababababababaababab\n",
      "abababababababababababababababababaabab\n",
      "ababababababababababababababababababaab\n",
      "abababababababababababababababababababa\n",
      "\n",
      "I = 19, L = bbbbbbbbbbbbbbbbbbbaaaaaaaaaaaaaaaaaaaa\n",
      "\n",
      "DECODING\n",
      "T = Positions of rows of M' in M: [20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n",
      "Tx = Positions in L of output symbols (reversed) = [19, 0, 20, 1, 21, 2, 22, 3, 23, 4, 24, 5, 25, 6, 26, 7, 27, 8, 28, 9, 29, 10, 30, 11, 31, 12, 32, 13, 33, 14, 34, 15, 35, 16, 36, 17, 37, 18, 38]\n",
      "S = abababababababababababababababababababa\n",
      "\n",
      "ENCODING (run are formed by the symbol that define a context)\n",
      "Original string:\n",
      "S = abacadaeaf\n",
      "\n",
      "Permutations matrix:\n",
      "abacadaeaf 0\n",
      "bacadaeafa 1\n",
      "acadaeafab 2\n",
      "cadaeafaba 3\n",
      "adaeafabac 4\n",
      "daeafabaca 5\n",
      "aeafabacad 6\n",
      "eafabacada 7\n",
      "afabacadae 8\n",
      "fabacadaea 9\n",
      "\n",
      "Sorted matrix:\n",
      "abacadaeaf 0\n",
      "acadaeafab 1\n",
      "adaeafabac 2\n",
      "aeafabacad 3\n",
      "afabacadae 4\n",
      "bacadaeafa 5\n",
      "cadaeafaba 6\n",
      "daeafabaca 7\n",
      "eafabacada 8\n",
      "fabacadaea 9\n",
      "\n",
      "M' = M rotated one character to the right\n",
      "fabacadaea\n",
      "bacadaeafa\n",
      "cadaeafaba\n",
      "daeafabaca\n",
      "eafabacada\n",
      "abacadaeaf\n",
      "acadaeafab\n",
      "adaeafabac\n",
      "aeafabacad\n",
      "afabacadae\n",
      "\n",
      "I = 0, L = fbcdeaaaaa\n",
      "\n",
      "DECODING\n",
      "T = Positions of rows of M' in M: [9, 5, 6, 7, 8, 0, 1, 2, 3, 4]\n",
      "Tx = Positions in L of output symbols (reversed) = [0, 9, 4, 8, 3, 7, 2, 6, 1, 5]\n",
      "S = abacadaeaf\n",
      "\n",
      "ENCODING (or by the symbol that goes after one or more contexts)\n",
      "Original string:\n",
      "S = bacadaeafa\n",
      "\n",
      "Permutations matrix:\n",
      "bacadaeafa 0\n",
      "acadaeafab 1\n",
      "cadaeafaba 2\n",
      "adaeafabac 3\n",
      "daeafabaca 4\n",
      "aeafabacad 5\n",
      "eafabacada 6\n",
      "afabacadae 7\n",
      "fabacadaea 8\n",
      "abacadaeaf 9\n",
      "\n",
      "Sorted matrix:\n",
      "abacadaeaf 0\n",
      "acadaeafab 1\n",
      "adaeafabac 2\n",
      "aeafabacad 3\n",
      "afabacadae 4\n",
      "bacadaeafa 5\n",
      "cadaeafaba 6\n",
      "daeafabaca 7\n",
      "eafabacada 8\n",
      "fabacadaea 9\n",
      "\n",
      "M' = M rotated one character to the right\n",
      "fabacadaea\n",
      "bacadaeafa\n",
      "cadaeafaba\n",
      "daeafabaca\n",
      "eafabacada\n",
      "abacadaeaf\n",
      "acadaeafab\n",
      "adaeafabac\n",
      "aeafabacad\n",
      "afabacadae\n",
      "\n",
      "I = 5, L = fbcdeaaaaa\n",
      "\n",
      "DECODING\n",
      "T = Positions of rows of M' in M: [9, 5, 6, 7, 8, 0, 1, 2, 3, 4]\n",
      "Tx = Positions in L of output symbols (reversed) = [5, 0, 9, 4, 8, 3, 7, 2, 6, 1]\n",
      "S = bacadaeafa\n",
      "\n",
      "ENCODING ( see https://link.springer.com/book/10.1007/978-0-387-78909-5 )\n",
      "Original string:\n",
      "S = aardvark$\n",
      "\n",
      "Permutations matrix:\n",
      "aardvark$ 0\n",
      "ardvark$a 1\n",
      "rdvark$aa 2\n",
      "dvark$aar 3\n",
      "vark$aard 4\n",
      "ark$aardv 5\n",
      "rk$aardva 6\n",
      "k$aardvar 7\n",
      "$aardvark 8\n",
      "\n",
      "Sorted matrix:\n",
      "$aardvark 0\n",
      "aardvark$ 1\n",
      "ardvark$a 2\n",
      "ark$aardv 3\n",
      "dvark$aar 4\n",
      "k$aardvar 5\n",
      "rdvark$aa 6\n",
      "rk$aardva 7\n",
      "vark$aard 8\n",
      "\n",
      "M' = M rotated one character to the right\n",
      "k$aardvar\n",
      "$aardvark\n",
      "aardvark$\n",
      "vark$aard\n",
      "rdvark$aa\n",
      "rk$aardva\n",
      "ardvark$a\n",
      "ark$aardv\n",
      "dvark$aar\n",
      "\n",
      "I = 1, L = k$avrraad\n",
      "\n",
      "DECODING\n",
      "T = Positions of rows of M' in M: [5, 0, 1, 8, 6, 7, 2, 3, 4]\n",
      "Tx = Positions in L of output symbols (reversed) = [1, 0, 5, 7, 3, 8, 4, 6, 2]\n",
      "S = aardvark$\n"
     ]
    }
   ],
   "source": [
    "# https://gist.github.com/dmckean/9723bc06254809e9068f\n",
    "\n",
    "def BWT(S):\n",
    "    \n",
    "    if __debug__:\n",
    "        print('Original string:')\n",
    "        print(\"S =\", S)\n",
    "        \n",
    "    n = len(S)\n",
    "\n",
    "    N = [S[i:n]+S[0:i] for i in range(n)]\n",
    "\n",
    "    if __debug__:\n",
    "        print('')\n",
    "        print('Permutations matrix:')\n",
    "        counter = 0\n",
    "        for i in N:\n",
    "            print(i, counter)\n",
    "            counter += 1\n",
    "\n",
    "    # 1. Matrix of all possible rotations (cyclid shifts) of 's'.\n",
    "    M = sorted(N)\n",
    "    \n",
    "    if __debug__:\n",
    "        print('')\n",
    "        print('Sorted matrix:')\n",
    "        counter = 0\n",
    "        for i in M:\n",
    "            print(i, counter)\n",
    "            counter += 1\n",
    "\n",
    "    # 2. I = the index of 's' in 'M'.\n",
    "    I = M.index(S) \n",
    "    \n",
    "    # 3. L = the last column of 'M'.\n",
    "    L = ''.join([q[-1] for q in M])\n",
    "\n",
    "    if __debug__:\n",
    "        print('')\n",
    "        print('M\\' = M rotated one character to the right')\n",
    "        Mp = []\n",
    "        for i in range(n):\n",
    "            Mp.append(M[i][n-1:n]+M[i][0:n-1])\n",
    "        for i in range(n):\n",
    "            print(Mp[i])\n",
    "    \n",
    "    return (I, L)\n",
    "\n",
    "from operator import itemgetter\n",
    "\n",
    "def iBWT(I, L):\n",
    "    n = len(L)\n",
    "    \n",
    "    # 1. Compute correspondence between the rows of M and M'.\n",
    "    X = sorted([(i, x) for i, x in enumerate(L)], key=itemgetter(1))\n",
    "    T = [None for i in range(n)]\n",
    "    for i, y in enumerate(X):\n",
    "        j, _ = y\n",
    "        T[j] = i\n",
    "        \n",
    "    if __debug__:\n",
    "        print(\"T = Positions of rows of M\\' in M:\", T)\n",
    "\n",
    "    # 2. for i in range(n): S[n-1-i] = L[T^i[I]]\n",
    "    # where T^0[x]=x and T^{i+1}[x] = T[T^i[x]].\n",
    "    Tx = [I]\n",
    "    for i in range(1, n):\n",
    "        Tx.append(T[Tx[i-1]])\n",
    "    if __debug__:\n",
    "        print(\"Tx = Positions in L of output symbols (reversed) =\", Tx)\n",
    "    S = [L[i] for i in Tx]\n",
    "    S.reverse()\n",
    "    return ''.join(S)\n",
    "\n",
    "print(\"ENCODING\")\n",
    "I, L = BWT('abraca.')\n",
    "print(\"\")\n",
    "print (\"I = {}, L = {}\\n\".format(I, L))\n",
    "print(\"DECODING\")\n",
    "Sp = iBWT(I, L)\n",
    "print (\"S = {}\".format(Sp))\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "print(\"ENCODING\")\n",
    "I, L = BWT('abababababababababababababababababababa')\n",
    "print(\"\")\n",
    "print (\"I = {}, L = {}\\n\".format(I, L))\n",
    "print(\"DECODING\")\n",
    "Sp = iBWT(I, L)\n",
    "print (\"S = {}\".format(Sp))\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "print(\"ENCODING (run are formed by the symbol that define a context)\")\n",
    "I, L = BWT('abacadaeaf')\n",
    "print(\"\")\n",
    "print (\"I = {}, L = {}\\n\".format(I, L))\n",
    "print(\"DECODING\")\n",
    "Sp = iBWT(I, L)\n",
    "print (\"S = {}\".format(Sp))\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "print(\"ENCODING (or by the symbol that goes after one or more contexts)\")\n",
    "I, L = BWT('bacadaeafa')\n",
    "print(\"\")\n",
    "print (\"I = {}, L = {}\\n\".format(I, L))\n",
    "print(\"DECODING\")\n",
    "Sp = iBWT(I, L)\n",
    "print (\"S = {}\".format(Sp))\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "print(\"ENCODING ( see https://link.springer.com/book/10.1007/978-0-387-78909-5 )\")\n",
    "I, L = BWT('aardvark$')\n",
    "print(\"\")\n",
    "print (\"I = {}, L = {}\\n\".format(I, L))\n",
    "print(\"DECODING\")\n",
    "Sp = iBWT(I, L)\n",
    "print (\"S = {}\".format(Sp))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## B. String encoding\n",
    "\n",
    "### How it works?\n",
    "\n",
    "* We replace strings by code-words of less length.\n",
    "* Strings are searched in a dictionary and the sequence of positions of the strings in the dictionary form the code-stream."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### B.1 LZ77 [[Ziv and Lempel, 1977]](https://scholar.google.es/scholar?hl=es&as_sdt=0%2C5&q=Ziv+Lempel+universal+sequential+data+compression+1977&btnG=)\n",
    "\n",
    "* Jacov Ziv and Abraham Lempel proposed the LZ77 algorithm in 1977. \n",
    "* In the eighties, [LZSS](https://en.wikipedia.org/wiki/Lempel%E2%80%93Ziv%E2%80%93Storer%E2%80%93Szymanski) (a branch of LZ77) was\n",
    "  implemented by Haruyasu Yoshizaki in the [LHA compressor](https://en.wikipedia.org/wiki/LHA_(file_format)), discovering\n",
    "  the possibilities of the LZ77 encoder.\n",
    "* After that, a large number of text compressors have been based\n",
    "  on LZ77 (or a variation of it). Some of the most famous\n",
    "  are: [ARJ](https://en.wikipedia.org/wiki/ARJ), [RAR](https://en.wikipedia.org/wiki/RAR_(file_format), [gzip](https://en.wikipedia.org/wiki/Gzip) and [7z](https://en.wikipedia.org/wiki/7z).\n",
    "* LZ77 processes a sequence of symbols using the structure:\n",
    "\n",
    "<img src=\"00-fundamentals/LZ77.svg\" style=\"width: 600px;\" align=\"middle\"/>\n",
    "\n",
    "* The dictionary and the look-ahead buffer have a fixed size and\n",
    "  can be considered as a sliding window moving over the symbols while they are coded.\n",
    "  In each iteration, the input of a new\n",
    "  symbols generates the output of the oldest ones, which become the\n",
    "  newest symbols of the dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Encoder\n",
    "\n",
    "1. Let $I$ the length of the dictionary and $J$ the length of the\n",
    "   buffer.\n",
    "2. Input the first $J$ symbols in the buffer.\n",
    "3. While the input is not exhausted:\n",
    "    1. Let $i$ the position in the dictionary of the first $j$\n",
    "    symbols of the buffer and $k$ the symbol that makes that $j$ can\n",
    "    not be larger.\n",
    "    2. Output $ijk$.\n",
    "    3. Input the next $j+1$ in the buffer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Example\n",
    "\n",
    "<img src=\"00-fundamentals/LZ77_encoding_example.svg\" style=\"width: 800px;\" align=\"middle\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Decoder\n",
    "\n",
    "1. While the code-words $ijk$ are not exhausted:\n",
    "    1. Output the $j$ symbols extracted from the position $i$ in the\n",
    "    dictionary.\n",
    "    2. Output $k$.\n",
    "    3. Introduce all the decoded symbols into the buffer.\n",
    "\n",
    "### Example\n",
    "\n",
    "<img src=\"00-fundamentals/LZ77_decoding_example.svg\" style=\"width: 500px;\" align=\"middle\"/>\n",
    "\n",
    "* Parameters $I$ and $J$ control the performance\n",
    "  of the algorithm. They should be large enough to guarantee the\n",
    "  matching of long strings, but should keep small in order to reduce\n",
    "  the number of bits of the code-words $ijk$. Typical sizes are:\n",
    "  $\\log_2(I)=12.0$ and $\\log_2(J)=4.0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To-do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### B.2 LZ78 [[Ziv and Lempel, 1978]](https://scholar.google.es/scholar?hl=es&as_sdt=0%2C5&q=Ziv+Lempel+1978&btnG=)\n",
    "\n",
    "* In 1978, Ziv and Lempel published the [LZ78 algorithm](https://en.wikipedia.org/wiki/LZ77_and_LZ78).\n",
    "\n",
    "* LZ78 represents the dictionary in a recursive way with the idea\n",
    "  of improving the search of the strings in the dictionary. Now, each\n",
    "  entry in the dictionary is a pair $wk$, where $w$ is a pointer to\n",
    "  the dictionary and $k$ is a symbol. In fact, each entry $wk$\n",
    "  represents the string that results from the concatenation of string\n",
    "  $w$ and $k$, where $w$ can be recursively computed in the same way\n",
    "  we have found $wk$.\n",
    "  \n",
    "* We will denote the string that $w$ represents by *string*$(w)$.\n",
    "  \n",
    "* The empty string is obtained by *string*$(0)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Encoder\n",
    "\n",
    "1. $w\\leftarrow 0$.\n",
    "2. While the input is not exhausted:\n",
    "    1. $k\\leftarrow$ next input symbol.\n",
    "    2. If $wk$ is found in the dictionary, then:\n",
    "        1. $w\\leftarrow$ address of $wk$ in the dictionary.\n",
    "    3. Else:\n",
    "        1. Output $wk$.\n",
    "        2. Insert $wk$ in the dictionary.\n",
    "        3. $w\\leftarrow 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Example\n",
    "\n",
    "<img src=\"00-fundamentals/LZ78_encoding_example.svg\" style=\"width: 700px;\" align=\"middle\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Decoder\n",
    "\n",
    "1. While the input is not exhausted:\n",
    "    1. Input $wk$.\n",
    "    2. Output $\\text{string}(w)$.\n",
    "    3. Output $k$.\n",
    "    4. Insert $wk$ in the dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Example\n",
    "\n",
    "<img src=\"00-fundamentals/LZ78_decoding_example.svg\" style=\"width: 700px;\" align=\"middle\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TO-DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### B.3 LZW [[Welch, 1984]](https://scholar.google.es/scholar?hl=es&as_sdt=0%2C5&q=Terry+Welch+1984&btnG=)\n",
    "\n",
    "* In 1984, Terry A. Welch proposed the [LZW algorithm](https://en.wikipedia.org/wiki/Lempel%E2%80%93Ziv%E2%80%93Welch),\n",
    "  an improved version of the LZ78 algorithm that does not\n",
    "  writes raw symbols ($k$) to the code-stream.\n",
    "\n",
    "* LZW was selected as encoding the engine for [GIF (Graphics\n",
    "  Interchange Format)](https://en.wikipedia.org/wiki/GIF), and for the compressor [`compress`](https://en.wikipedia.org/wiki/Compress).\n",
    "  \n",
    "* The dictionary is initially filled with the $2^k$ possible\n",
    "  symbols (*roots*), that are stored in entries $0\\cdots255$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Encoder\n",
    "\n",
    "1. $w\\leftarrow$ next input symbol.\n",
    "2. While the input is not exhausted:\n",
    "    1. $k\\leftarrow$ next input symbol.\n",
    "    2. If $wk$ is found in the dictionary, then:\n",
    "        1. $w\\leftarrow$ address of $wk$ in the dictionary.\n",
    "    3. Else:\n",
    "        1. Output $\\leftarrow w$.\n",
    "        2. Insert $wk$ in the dictionary.\n",
    "        3. $w\\leftarrow k$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Example\n",
    "\n",
    "<img src=\"00-fundamentals/LZW_encoding_example.svg\" style=\"width: 800px;\" align=\"middle\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Decoder\n",
    "\n",
    "1. $code\\leftarrow$ first input code-word.\n",
    "2. Output $code$.\n",
    "3. $old\\_code\\leftarrow code$.\n",
    "4. While the input is not exhausted:\n",
    "    1. $code\\leftarrow$ next input code-word.\n",
    "    2. $w\\leftarrow old\\_code$.\n",
    "    3. If $code$ is found in the dictionary, then:\n",
    "        1. Output string$(code)$.\n",
    "    4. Else:\n",
    "        1. Output string$(w)$.\n",
    "        2. Output $k$.\n",
    "    5. $k\\leftarrow$ first symbol of the last output.\n",
    "    6. Insert $wk$ in the dictionary.\n",
    "    7. $old\\_code\\leftarrow code$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Example\n",
    "\n",
    "<img src=\"00-fundamentals/LZW_decoding_example.svg\" style=\"width: 400px;\" align=\"middle\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://rosettacode.org/wiki/LZW_compression#Python\n",
    "\n",
    "# TO-DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## C. Symbol encoding\n",
    "\n",
    "### How it works?\n",
    "\n",
    "* We can compress if each symbol is translated by code-words and,\n",
    "  in average, the lengths of the code-words are smaller than the\n",
    "  length of the symbols.\n",
    "  \n",
    "* The encoder and the decoder have a probabilistic model $M$ which\n",
    "  says to the variable-length encoder ($C$)/decoder($C^{-1}$) the\n",
    "  probability $p(s)$ of each symbol $s$.\n",
    "  \n",
    "<img src=\"00-fundamentals/compresion_entropica.svg\" style=\"width: 600px;\" align=\"middle\"/>\n",
    "\n",
    "* The most probable symbols are represented by the shorter\n",
    "  code-words and viceversa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bits, data, information ...\n",
    "\n",
    "* Data is the representation of the information.\n",
    "\n",
    "* Lossless data compression uses a shorter representation for the\n",
    "  information.\n",
    "  \n",
    "* By definition, a bit of data stores a bit of information, if and\n",
    "  only if, it represents the occurrence of an equiprobable event (an\n",
    "  event that can be true or false with the same probability).\n",
    "  In this ideal situation, the representation if fully efficient\n",
    "  (not compression would be possible).\n",
    "  \n",
    "* By definition, a symbol $s$ with probability $p(s)$ stores\n",
    "\\begin{equation}\n",
    "  I(s)=-\\log_2 p(s) \\tag{Eq:symbol_information}\n",
    "  \\label{Eq:symbol_information}\n",
    "\\end{equation}\n",
    "  bits of information.\n",
    "\n",
    "* The length of the code-word depends on the probability as:\n",
    "\n",
    "<img src=\"00-fundamentals/prob_vs_long.svg\" style=\"width: 600px;\" align=\"middle\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Entropy\n",
    "\n",
    "* The entropy $H(S)$ measures the amount of information per\n",
    "  symbol that a source of information $S$ produces, in average, i.e.,\n",
    "\\begin{equation}\n",
    "  H(S) = \\frac{1}{N}\\sum_{s=1}^{N} p(s)\\times I(s)\n",
    "\\end{equation}\n",
    "  bits-of-information/symbol, where $N$ is the size of the source\n",
    "  alphabet (number of different symbols)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### C.1 Universal coding\n",
    "\n",
    "* An ideal entropy encoder should represent each symbol $s$ with a\n",
    "  number of bits that $I(s)$ says (see Eq. Eq:symbol_information).\n",
    "  \n",
    "* This system will be 100% efficient is the guesses are\n",
    "  equiprobable!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Encoding of a symbol\n",
    "\n",
    "1. While the decoder does not know the symbol:\n",
    "    1. Assert something about the symbol that allows to the decoder\n",
    "    to minimize the uncertainty of finding that symbol. This guess\n",
    "    should have the same probability of to be true or false.\n",
    "    2. Output a bit of code that says if the last guess is true or\n",
    "    false.\n",
    "    \n",
    "### Decoding of a symbol\n",
    "\n",
    "1. While the symbol is not known without uncertainty:\n",
    "    1. Make the same guess that the encoder.\n",
    "    2. Input a bit of code that represents the result of the last\n",
    "    guess."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example\n",
    "\n",
    "* Let's suppose that we use the Spanish alphabet. Humans know that\n",
    "  symbols does not form words in any order (this fact can help us to\n",
    "  formulate the following VLC (Variable Length Codec)):\n",
    "  \n",
    "* In Spanish there are 28 letters. Therefore, to encode, for example,\n",
    "  the word \"preciosa\", the first symbol \"p\" can be represented by\n",
    "  it index inside the Spahish alphabet with a code-word of 5 bits. In\n",
    "  this try, the encoding is not a very efficient, but this one is the\n",
    "  first letter ... For the second one \"r\" we can see (using a\n",
    "  Spanish dictionary) that after a \"p\", the following symbols are\n",
    "  possible: (1) \"a\", (2) \"e\", (3) \"i\", (4) \"l\", (5) \"n\", (6)\n",
    "  \"o\", (7) \"r\", (8) \"s\" and (9) \"u\". Therefore, we don't need\n",
    "  5 bits now, 4 are enough.\n",
    "  \n",
    "<img src=\"00-fundamentals/universal_coding_example.svg\" style=\"width: 300px;\" align=\"middle\"/>\n",
    "\n",
    "* Notice that the compression ratio has been 40/25:1 (``preciosa'' has 8\n",
    "  letters)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### C.2 [Shannon-Fano coding](https://en.wikipedia.org/wiki/Shannon%E2%80%93Fano_coding) [[Shannon, 1948]](https://scholar.google.es/scholar?hl=es&as_sdt=0%2C5&q=Shannon+2001+A+Mathematical+Theory+of+Communication&btnG=),  [[Fano, 1949]](https://scholar.google.es/scholar?hl=es&as_sdt=0%2C5&q=Fano+1949+%22The+transmission+of+information%22&btnG=)\n",
    "\n",
    "* At the end of the 40's, Claude E. Shannon (Bell Labs) and\n",
    "  R.M. Fano (MIT) developed the following VLC codec.\n",
    "  \n",
    "### Encoder\n",
    "\n",
    "1. Sort the symbols using their probabilities.\n",
    "2. Split the set of symbols into two subsets in a way in which the\n",
    "   each subset have the same total probability.\n",
    "3. Assign a different bit to each set.\n",
    "4. Repeat the previous procedure to each subset until the size of\n",
    "   each subset is equal to 1.\n",
    "\n",
    "### Example\n",
    "\n",
    "* Let's use the next probabilistic model:\n",
    "<img src=\"00-fundamentals/shannon-fano_example.svg\" style=\"width: 120px;\" align=\"middle\"/>\n",
    "Using it, this is the coding:\n",
    "<img src=\"00-fundamentals/shannon-fano_example-coding.svg\" style=\"width: 850px;\" align=\"middle\"/>\n",
    "\n",
    "### Decoder\n",
    "\n",
    "TO-DO."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### C.3 [Huffman coding](https://en.wikipedia.org/wiki/Huffman_coding) [[Huffman, 1952]](https://scholar.google.es/scholar?hl=es&as_sdt=0%2C5&q=huffman+method+codes+1952&btnG=)\n",
    " \n",
    "* Optimal performance (better than Shannon-Fano) when a integer\n",
    "  number of bits is assigned to each symbol.\n",
    "* The VLC codec builds a binary tree where the symbols are stored\n",
    "  in the leafs and the distance of each symbol to the root of the tree\n",
    "  is $\\lceil\\log_2(p(s))\\rceil$.\n",
    "* After label each binary branch in the tree, the Huffman\n",
    "  code-word for the symbol $s$ is the sequence of bits (labels) that\n",
    "  we must use to travel from the root to the $s$-leaf.\n",
    " \n",
    "### Building Huffman trees\n",
    "\n",
    "1. Create a list of nodes. Each node stores a symbol and its\n",
    "   probability.\n",
    "2. While the number of nodes in the list > 1:\n",
    "    1. Extract from the list the 2 nodes with the lowest probability.\n",
    "    2. Insert in the list a new node (that is the root of a binary\n",
    "       tree) whose probability is the sum of the probability of its\n",
    "       leafs.\n",
    "       \n",
    "### Example\n",
    "\n",
    "<img src=\"00-fundamentals/huffman_ejemplo.svg\" style=\"width: 400px;\" align=\"middle\"/>\n",
    "\n",
    "### Encoder\n",
    "TO-DO\n",
    "\n",
    "### Example\n",
    "TO-DO\n",
    "\n",
    "### Decoder\n",
    "TO-DO\n",
    "\n",
    "### Example\n",
    "TO-DO\n",
    "\n",
    "### Limits\n",
    "\n",
    "* Any Huffman code satisfies that\n",
    "\n",
    "\\begin{equation}\n",
    "  l\\big(c(s)\\big) = \\lceil I(s)\\rceil, \\tag{Eq:Huffman} \\label{Eq:Huffman}\n",
    "\\end{equation}\n",
    "\n",
    "  where $l\\big(c(s)\\big)$ is the length of the code-word assigned to\n",
    "  the symbol $s$.\n",
    "\n",
    "* This implies that, with every encoded symbol, up to 1 bit of\n",
    "  redundant data can be introduced (think about a very frequent symbol).\n",
    "  \n",
    "* This is a problem that grows when the size of the alphabet is\n",
    "  small. In the extreme case, for binary source alphabets, the Huffman\n",
    "  coding does not change the length of the original representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### C.4 Arithmetic coding\n",
    "\n",
    "* Arithmetic coding relaxes the Eq. Eq:Huffman,\n",
    "  verifying that, for every encoded symbol, \n",
    "  \\begin{equation}\n",
    "    l\\big(c(s)\\big) = I(s), \\tag{Eq:arithmetic}\n",
    "    \\label{eq:arithmetic}\n",
    "  \\end{equation}\n",
    "  i.e. the number of bits of data (code-word) assigned by the encoder\n",
    "  is equal to the number of bits of information that the symbol\n",
    "  represent.\n",
    "\n",
    "<img src=\"00-fundamentals/comparacion.svg\" style=\"width: 800px;\" align=\"middle\"/>\n",
    "\n",
    "* It can be said that, arithmetic coding is optimal because\n",
    "  the average length of an arithmetic code is equal to the entropy of\n",
    "  the information source, measured in bits/symbol.\n",
    "\n",
    "### An ideal encoder\n",
    "\n",
    "1. Let $[L,H)\\leftarrow [0.0,1.0)$ an interval of real numbers.\n",
    "2. While the input is not exhausted:\n",
    "    1. Split $[L,H)$ into so many sub-intervals as different symbols\n",
    "       are in the alphabet. The size of each sub-interval is proportional\n",
    "       to the probability of the corresponding symbol.\n",
    "    2. Select the sub-interval $[L',H')$ associated with the encoded\n",
    "       symbol.\n",
    "    3. $[L,H)\\leftarrow [L',H')$.\n",
    "3. Output a real number $x\\in[L,H)$ (the arithmetic\n",
    "   code-stream). The number of decimals of $x$ should be large enough\n",
    "   to distinguish the final sub-interval $[L,H)$ from the rest of\n",
    "   possibilities.\n",
    "   \n",
    "### Example\n",
    "\n",
    "* Imagine a binary sequence, where $p(\\text{A})=3/4$ and\n",
    "  $p(\\text{B})=1/4$. Compute the arithmetic code of the sequences A, B,\n",
    "  AA, AB, BA y BB.\n",
    "  \n",
    "<img src=\"00-fundamentals/aritmetica_ejemplo.svg\" style=\"width: 500px;\" align=\"middle\"/>\n",
    "\n",
    "### An ideal decoder\n",
    "\n",
    "1. Let $[L,H)\\leftarrow [0.0,1.0)$ the initial interval.\n",
    "2. While the input is not exhausted:\n",
    "    1. Split $[L,H)$ into so many sub-intervals as different symbols\n",
    "       are in the alphabet. The size of each sub-interval is proportional\n",
    "       to the probability of the corresponding symbol.\n",
    "    2. Input so many bits of $x$ as they are needed to:\n",
    "        1. Select the sub-interval $[L',H')$ that contains $x$.\n",
    "        2. Output the symbol that $[L',H')$ represents.\n",
    "        3. $[L,H)\\leftarrow[L',H')$.\n",
    "        \n",
    "### Example\n",
    "TO-DO\n",
    "\n",
    "### Incremental transmission\n",
    "\n",
    "* It is not necessary to wait for the end of the encoding to\n",
    "  generate the arithmetic code. When we work with binary\n",
    "  representations of the real numbers $L$ and $H$, their most\n",
    "  significant bits become identical when the interval is reduced. These\n",
    "  bits belong to the output arithmetic code, therefore, they\n",
    "  can be output as soon as they are known.\n",
    "  \n",
    "  For example, when the symbol B is encoded, a code-bit 1 can be\n",
    "    output because any sequence of symbols that start with B have a\n",
    "    code-word that begins by 1.\n",
    "    \n",
    "* When the most significant bits of $L$ and $H$ are output, the\n",
    "  bits of each register are shifted to the left, and new bits need to\n",
    "  be inserted. The results is an automatic zoom of the selected\n",
    "  sub-interval.\n",
    "\n",
    "  Following with the previous example, the register shifting generates\n",
    "    an ampliation of the $[0.50,1.00)$ interval to the $[0.00,1.00)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Lab\n",
    "TO-DO."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## C.5 Probabilistic models\n",
    "\n",
    "* In order to use any of the previous VLCs, a probabilistic model is always needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### C.5.1 Static models\n",
    "\n",
    "* The simplest models because the probabilities of the symbols\n",
    "  remain constant.\n",
    "* The variable-length codec can be precomputed.\n",
    "* If the last premise is true, the entropy codec is efficient an\n",
    "  fast. For this reason, static models are very common in codecs such\n",
    "  as JPEG, MPEG (audio and video), etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### C.5.2 Adaptive models\n",
    "\n",
    "* The probabilities of the symbols are computed in run-time.\n",
    "* In general, the compression ratios that an adaptive model \n",
    "  get are better than the static model's ones because the\n",
    "  probabilities of the symbols are localy computed.\n",
    "  \n",
    "### Encoding\n",
    "\n",
    "1. Asign the same probability to all the symbols.\n",
    "2. While the input if not exhausted:\n",
    "    1. Encode the next symbol.\n",
    "    2. Update (increase) its probability.\n",
    "    \n",
    "### Example\n",
    "TO-DO\n",
    "\n",
    "### Decoding\n",
    "\n",
    "1. Identical to the step 1 of the encoder.\n",
    "2. While the input is not exhausted:\n",
    "    1. Decode the next symbol.\n",
    "    2. Identical to the step 2.b of the encoder.\n",
    "    \n",
    "### Example\n",
    "TO-DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### C.5.3 Initially empty models\n",
    "\n",
    "* The smaller the number of symbols used by the model, the higher\n",
    "  the probabilities, and therefore, the better the compression ratios.\n",
    "* An initially empty model only stores the ESC(cape) symbol, a\n",
    "  symbol that it is used by the encoder only when a new symbol is\n",
    "  found.\n",
    "\n",
    "### Encoder\n",
    "\n",
    "1. Set the probability of the ESC to $1.0$ (and the probability of\n",
    "   the rest of the symbols to $0.0$).\n",
    "2. While the input is not exhausted:\n",
    "    1. $s\\leftarrow$ next symbol.\n",
    "    2. If $s$ has been found before, then:\n",
    "        1. Encode $s$ and output $c(s)$.\n",
    "    3. Else:\n",
    "        1. Output $c(\\mathrm{ESC})$.\n",
    "        2. Output a raw symbol $s$.\n",
    "    4. Update $p(s)$.\n",
    "\n",
    "### Example\n",
    "TO-DO\n",
    "\n",
    "### Decoder\n",
    "\n",
    "1. Identical to the step 1 of the encoder.\n",
    "2. While the input is not exhausted:\n",
    "    1. $c(s)\\leftarrow $ next code-word.\n",
    "    2. Decode $s$.\n",
    "    3. If $s=$ ESC, then:\n",
    "        1. Input a raw symbol $s$.\n",
    "    4. Update $p(s)$.\n",
    "    5. Output $s$.\n",
    "    \n",
    "### Example\n",
    "TO-DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "   \n",
    "### C.5.4 Models with memory\n",
    "\n",
    "* In most cases, the probability of a symbol depends on its\n",
    "  neighborhood (context).\n",
    "* The higher the memory of the model (the context), the higher the\n",
    "  accuracy of the predictions (probabilities), and therefore, the\n",
    "  lower the length of the code-words \\cite{Cleary.PPM}.\n",
    "* Let ${\\cal C}[i]$ the last $i$ encoded symbols and\n",
    "  $p(s|{\\cal C}[i])$ the probability that the symbol $s$ follows\n",
    "  the context ${\\cal C}[i]$.\n",
    "* Let $k$ the maximal order of the prediction (i.e. the largest\n",
    "  number of symbols of ${\\cal C}[]$ that are going to be used as the\n",
    "  actual context). Notice that ${\\cal C}[0]=\\varnothing$ and the model\n",
    "  has no memory.\n",
    "* We suppose that arithmetic coding is used and therefore, when we\n",
    "  input or output $c(s)$, we are transmitting $I(s)$ bits of code.\n",
    "* Let $r$ the size of the source alphabet.\n",
    "\n",
    "### Encoder\n",
    "\n",
    "1. Create an empty model for every context $0\\le i \\le k$.\n",
    "2. Create an non-empty model for $k=-1$.\n",
    "3. While the input is not exhausted:\n",
    "    1. $s\\leftarrow$ Input$_{\\log_2(r)}$.\n",
    "    2. $i\\leftarrow k$ (except for the first symbol, where\n",
    "       $i\\leftarrow 0$).\n",
    "    3. While $p(s|{\\cal C}[i])=0$ (it is the first time that $s$ follows\n",
    "       ${\\cal C}[i]$):\n",
    "        1. Output $\\leftarrow c(\\text{ESC}|{\\cal C}[i])$.\n",
    "        2. Update $p(\\text{ESC}|{\\cal C}[i])$.\n",
    "        3. Update $p(s|{\\cal C}[i])$ (insert $s$ into the ${\\cal C}[i]$ context).\n",
    "        4. $i\\leftarrow i-1$.\n",
    "    4. Output $\\leftarrow c(s|{\\cal C}[i])$. The symbols that were in\n",
    "       contexts with order $>i$ must be excluded of the actual (${\\cal C}[i]$) context because $s$ is not none of them.\n",
    "    5. If $i\\ge 0$, update $p(s|{\\cal C}[i])$.\n",
    "    \n",
    "### Example\n",
    "\n",
    "* Let $r=256$ the size of the source alphabet.\n",
    "\n",
    "* The probabilistic model $M[{\\cal C}[-1]]$ (for the special context\n",
    "  ${\\cal C}[-1]$) is non adaptative, non empty and has an special symbol EOF\n",
    "  (End Of File) that is going to be used when the compression has\n",
    "  finished:\n",
    "  $$M[{\\cal C}[-1]]=\\{0,1~1,1~\\cdots~\\mathtt{a},1~\\mathtt{b},1~\\cdots~255,1~\\text{EOF},1\\}.$$\n",
    "  In a pair $a,b$, $a$ is the symbol and $b$ is its probability (counts).\n",
    "\n",
    "* $M[{\\cal C}[0]]$ is adaptative and empty:\n",
    "  $$M[{\\cal C}[0]]=\\{\\text{ESC},1\\}.$$\n",
    "\n",
    "* In this example (for the sake of the simplicity), the maximal\n",
    "  order of the prediction $k=1$ (we only remember the previous\n",
    "  symbol). Therefore, there are $r=256$ probabilistic models:\n",
    "  $$M[{\\cal C}[1]]=\\{\\text{ESC},1\\}, 0\\le {\\cal C}[1]\\le r.$$\n",
    "  \n",
    "* Encoding of the first symbol (\\texttt{a}) (see Figure~\\ref{fig:PPM}):\n",
    "\n",
    "1. [3.A] $s\\leftarrow$ \\texttt{a}.\n",
    "2. [3.B] $i\\leftarrow 0$ (we don't know the previous symbol).\n",
    "3. [3.C] $p(\\mathtt{a}|{\\cal C}[0])=0$ (the context only has the ESC).\n",
    "4. [3.C.a] Output $\\leftarrow c(\\text{ESC}|{\\cal C}[0])$ (althought\n",
    "    $l(c(\\text{ESC}|{\\cal C}[0]))=0$).\n",
    "5. [3.C.b] Update $p(\\text{ESC}|{\\cal C}[0])$ (now, the count of ESC is\n",
    "    2).\n",
    "6. [3.C.c] Insert \\texttt{a} into\n",
    "    $M[{\\cal C}[0]]=\\{\\mathsf{ESC},2~\\mathtt{a},1\\}$.\n",
    "7. [3.C.d] $i\\leftarrow -1$.\n",
    "8. [3.c] $p(\\mathtt{a}|{\\cal C}[-1])\\neq 0$.\n",
    "9. [3.d] Output $\\leftarrow c(\\texttt{a}|{\\cal C}[-1])$ where\n",
    "    $p(\\texttt{a}|{\\cal C}[-1]) = 1/(256+1)$.\n",
    "    \n",
    "* Encoding of the second symbol (\\texttt{b}):\n",
    "\n",
    "1. [3.a] $s\\leftarrow$ \\texttt{b}.\n",
    "2. [3.b] $i\\leftarrow 1$.\n",
    "3. [3.c] $p(\\mathtt{b}|{\\cal C}[1])=0$ because ${\\cal C}[1]=\\texttt{a}$ and\n",
    "   $M[\\texttt{a}]=\\{\\text{ESC},1\\}$.\n",
    "4. [3.c.i] Output $\\leftarrow c(\\text{ESC}|\\texttt{a})$ (althought\n",
    "   $l(c(\\text{ESC}|\\texttt{a}))=0$).\n",
    "5. [3.c.ii] Update $p(\\text{ESC}|\\texttt{a})$ (now, the count of ESC is 2).\n",
    "6. [3.c.iii] Insert \\texttt{b} into $M[\\texttt{a}]=\\{\\text{ESC},2~ \\texttt{b},1\\}$.\n",
    "7. [3.c.iv] $i\\leftarrow 0$.\n",
    "8. [3.c] $p(\\mathtt{b}|{\\cal C}[0])=0$ because\n",
    "   $M[{\\cal C}[0]]=\\{\\mathsf{ESC},2~\\texttt{a},1\\}$.\n",
    "9. [3.c.i] Output $\\leftarrow c(\\text{ESC}|{\\cal C}[0])$ where\n",
    "   $p(\\text{ESC}|{\\cal C}[0]) = 2/3$.\n",
    "10. [3.c.ii] Update $p(\\text{ESC}|{\\cal C}[0])$ (now, the count of ESC is\n",
    "    3).\n",
    "11. [3.c.iii] Insert \\texttt{b} into $M[{\\cal C}[0]] = \\{\\text{ESC},3~\n",
    "    \\texttt{a},1~ \\texttt{b},1\\}$.\n",
    "12. [3.c.iv] $i\\leftarrow -1$.\n",
    "13. [3.c] $p(\\mathtt{b}|{\\cal C}[-1])\\neq 0$.\n",
    "14. [3.d] Output $\\leftarrow c(\\texttt{b}|{\\cal C}[-1])$ where\n",
    "    $p(\\mathtt{b}|{\\cal C}[-1]) = 1/r$. The symbol \\texttt{a} has been\n",
    "    excluded in the calculus of the probability of \\texttt{b} because\n",
    "    $\\texttt{a}\\in M[{\\cal C}[0]] = \\{\\text{ESC},3~ \\texttt{a},1~\n",
    "    \\texttt{b},1\\}$.\n",
    "\n",
    "<img src=\"00-fundamentals/PPM_example.svg\" style=\"width: 800px;\" align=\"middle\"/>\n",
    "\n",
    "### Decoder\n",
    "\n",
    "1. Equal to the step 1 of the encoder.\n",
    "2. While the input is not exhausted:\n",
    "    1. $i\\leftarrow k$ (except for the first symbol, where $i\\leftarrow 0$).\n",
    "    2. $s\\leftarrow$ next decoded symbol.\n",
    "    3. While $s=\\text{ESC}$:\n",
    "        1. Update $p(\\text{ESC}|{\\cal C}[i])$.\n",
    "        2. $i\\leftarrow i-1$.\n",
    "        3. $s\\leftarrow$ next decoded symbol.\n",
    "    4. Update $p(s|{\\cal C}[i])$.\n",
    "    5. While $i<k$:\n",
    "        1. $i\\leftarrow i+1$.\n",
    "        2. Update $p(s|{\\cal C}[i])$ (insert $s$ into the ${\\cal C}[i]$ context).\n",
    "        \n",
    "### Lab\n",
    "TO-DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## C.6 MTF (Move To Front) transform\n",
    "\n",
    "* Inputs a sequence of symbols and outputs a sequence of symbols.\n",
    "\n",
    "* The size (in bits of data) for each sequence is the same.\n",
    "\n",
    "* The entropy of the output is lower that the input's one.\n",
    "\n",
    "* Performs a change in the representation of the symbols where\n",
    "  those symbols that have a high probability of occurrency are\n",
    "  ``moved'' in the source alphabet towards decreasing positions.\n",
    "\n",
    "* The probability density function follows an exponential\n",
    "  distribution with a slope $\\lambda$ where\n",
    "\\begin{equation}\n",
    "  f(x.\\lambda) = \\left\\{ \\begin{array}{ll}\n",
    "      \\lambda e^{-\\lambda x} & \\mbox{if $x \\geq 0$};\\\\\n",
    "      0 & \\mbox{if $x < 0$}.\\end{array} \\right.\n",
    "\\end{equation}\n",
    "\n",
    "<img src=\"00-fundamentals/exponential.svg\" style=\"width: 600px;\" align=\"middle\"/>\n",
    "\n",
    "### Forward transform\n",
    "\n",
    "1. Create a list $L$ with the symbols of the source alphabet\n",
    "  where $$L[s]\\leftarrow s; 0\\le s\\le r.$$\n",
    "2. While the input is not exhausted:\n",
    "    1. $s\\leftarrow$ next input symbol.\n",
    "    2. $c\\leftarrow$ position of $s$ in $L$ ($L[c]=s$).\n",
    "    3. Output $\\leftarrow c$.\n",
    "    4. Move $s$ to the front of $L$.\n",
    "    \n",
    "### Example\n",
    "Not copied?\n",
    "    \n",
    "### Inverse transform\n",
    "\n",
    "1. The step 1 of the forward transform.\n",
    "2. While the input is not exausted:\n",
    "    1. $c\\leftarrow$ next input code.\n",
    "    2. $s\\leftarrow L[c]$.\n",
    "    3.  Output $s$.\n",
    "    4. The step 2.C of the forward transform.\n",
    "    \n",
    "### Example\n",
    "\n",
    "<img src=\"00-fundamentals/MTF_example.svg\" style=\"width: 250px;\" align=\"middle\"/>\n",
    "\n",
    "### Lab\n",
    "TO-DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## C.7 Context-based Text Predictive transform\n",
    "\n",
    "* The MTF uses a model where a symbol that has happened only one\n",
    "  time can get a index-code that is lower than the index-code of a\n",
    "  symbol that has been found thousands of times :-(\n",
    "\n",
    "* We can solve this problem if the positions of the symbols are\n",
    "  determined by their probability. In other words, the list $L$ will\n",
    "  be sorted by the ocurrence of the symbols.\n",
    "  \n",
    "### 0-order encoder\n",
    "\n",
    "1. The step 1 of the MTF transform, although now every node of the\n",
    "   list stores also a count of the symbol.\n",
    "2. While the input is not exhausted:\n",
    "    1. $s\\leftarrow$ next input symbol.\n",
    "    2. $c\\leftarrow$ position of $s$ in $L$ (the prediction error).\n",
    "    3. Output $\\leftarrow c$.\n",
    "    4. Update the count of $L[c]$ (the count of $s$) and keep sorted $L$.\n",
    "\n",
    "### Example\n",
    "\n",
    "<img src=\"00-fundamentals/TPT_example.svg\" style=\"width: 450px;\" align=\"middle\"/>\n",
    "\n",
    "### 0-order decoder\n",
    "\n",
    "1. The step 1 of the encoder.\n",
    "2. While the input is not exhausted:\n",
    "    1. $c\\leftarrow$ next input code.\n",
    "    2. $s\\leftarrow L[c]$.\n",
    "    3. Output $s$.\n",
    "    4. Step 2.D of the encoder.\n",
    "    \n",
    "### Example\n",
    "TO-DO\n",
    "    \n",
    "### $N$-order encoder\n",
    "\n",
    "1. Let ${\\cal C}[i]$ the context of $s$ and $L_{{\\cal C}[i]}$ the\n",
    "   list for that context. If $i>0$ then the lists are empty, else, the\n",
    "   list is full and the count of every node is $0$.\n",
    "2. Let $N$ the order of the prediction.\n",
    "3. Let $H=\\varnothing$ a list of tested symbols. All symbols in $H$\n",
    "   must be different.\n",
    "4. While the input is not exhausted:\n",
    "    1. $s\\leftarrow$ the next input symbol.\n",
    "    2. $i\\leftarrow k$ (except for the first symbol, where $i\\leftarrow 0$).\n",
    "    3. While $s\\notin L_{{\\cal C}[i]}$:\n",
    "        1. $H\\leftarrow \\text{reduce}(H\\cup L_{{\\cal C}[i]})$. (reduce$()$ deletes the repeated nodes).\n",
    "        2. Update the count of $s$ in $L_{{\\cal C}[i]}$ and keep sorted it.\n",
    "        3. $i\\leftarrow i-1$.\n",
    "    4. Let $c$ the position of $s$ en $L_{{\\cal C}[i]}$.\n",
    "    5. $c\\leftarrow c+$ symbols of $H-L_{{\\cal C}[i]}$. In this\n",
    "       way, the decoder will know the length of the context where $s$\n",
    "       happens and does not count the same symbol twice.\n",
    "    6. Output $\\leftarrow c$.\n",
    "    7. Update the count of $s$ in $L_{{\\cal C}[i]}$ and keep sorted it.\n",
    "    8. $H\\leftarrow\\varnothing$.\n",
    "    \n",
    "### Example ($k=1$)\n",
    "\n",
    "<img src=\"00-fundamentals/TPT_example.svg\" style=\"width: 450px;\" align=\"middle\"/>\n",
    "\n",
    "### $N$-order decoder\n",
    "\n",
    "1. Steps 1, 2 and 3 of the encoder.\n",
    "2. While the input is not exhausted:\n",
    "    1. $c\\leftarrow$ the next input code.\n",
    "    2. $i\\leftarrow k$ (except for the first symbol, where $i\\leftarrow 0$).\n",
    "    3. While $L_{{\\cal C}[i]}[c]=\\varnothing$:\n",
    "        1. $H\\leftarrow \\text{reduce}(H\\cup L_{{\\cal C}[i]})$.\n",
    "        2. $i\\leftarrow i-1$.\n",
    "    4. $s\\leftarrow \\text{reduce}(H\\cup L_{{\\cal C}[i]})[c]$.\n",
    "    5. Update the count of $L_{{\\cal C}[i]}[c]$.\n",
    "    6. While $i<k$:\n",
    "        1. $i\\leftarrow i+1$.\n",
    "        2. Insert the symbol $s$ in $L_{{\\cal C}[i]}$.\n",
    "        \n",
    "### Example\n",
    "TO-DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## C.8 Unary coding\n",
    "\n",
    "* It is a particular case of the Huffman code where the number of\n",
    "  bits of each code-word (minus one) is equal to the index of the\n",
    "  symbol in the source alphabet. Example:\n",
    "  \n",
    "<img src=\"00-fundamentals/Unary_example.svg\" style=\"width: 150px;\" align=\"middle\"/>\n",
    "\n",
    "* The unary coding is only optimal when (see Equation\n",
    "  \\ref{eq:symbol_information})\n",
    "  \\begin{equation}\n",
    "    p(s) = 2^{-(s+1)} \\tag{Eq:Unary}\n",
    "  \\end{equation}\n",
    "  where $s=0,1,\\cdots$.\n",
    "  \n",
    "<img src=\"00-fundamentals/unary.svg\" style=\"width: 800px;\" align=\"middle\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## C.9 Golomb coding [[Golomb, 1966]](https://scholar.google.es/scholar?hl=es&as_sdt=0%2C5&q=golomb+1966+run&btnG=)\n",
    "\n",
    "* When the probabilities of the symbols follow an exponential\n",
    "  distribution, the Golomg encoder has the same efficiency than the\n",
    "  Huffman coding, but it is faster. In this case, the probabilities of\n",
    "  the symbols shoud be\n",
    "  \n",
    "  \\begin{equation}\n",
    "    p(s) =\n",
    "    2^{\\displaystyle-\\Big(\\displaystyle m\\big\\lfloor\\displaystyle\\frac{s+m}{m}\\big\\rfloor\\Big)}\n",
    "    \\tag{Eq:Golomb}\n",
    "  \\end{equation}\n",
    "  where $s=0,1,\\cdots$ is the symbol and $m=0,1,\\cdots$ is the\n",
    "  ``Golomb slope'' of the distribution.\n",
    "  \n",
    "* For $m=2^k$, it is possible to find a very efficient\n",
    "  implementation and the encoder is also named Rice\n",
    "  encoder~\\cite{Rice79}. In this case\n",
    "  \n",
    "  \\begin{equation}\n",
    "    p(s) =\n",
    "    2^{\\displaystyle-\\Big(2^k \\displaystyle\\big\\lfloor\\displaystyle\\frac{s+2^k}{2^k}\\big\\rfloor\\Big)}\n",
    "    \\tag{Eq:Rice}\n",
    "    \\label{eq:Rice}\n",
    "  \\end{equation}\n",
    "\n",
    "<img src=\"00-fundamentals/Golomb_coding.svg\" style=\"width: 600px;\" align=\"middle\"/>\n",
    "\n",
    "* Notice that for $m=1$, we take the unary encoding.\n",
    "\n",
    "<img src=\"00-fundamentals/Golomb.svg\" style=\"width: 600px;\" align=\"middle\"/>\n",
    "\n",
    "### Encoder\n",
    "\n",
    "1. $k\\leftarrow \\lceil\\log_2(m)\\rceil$.\n",
    "2. $r\\leftarrow s~\\mathrm{mod}~m$.\n",
    "3. $t\\leftarrow 2^k-m$.\n",
    "4. Output $(s~\\mathrm{div}~m)$ using an unary code.\n",
    "5. If $r<t$:\n",
    "    1. Output the integer encoded in the $k-1$ least significant bits of $r$ using a binary code.\n",
    "6. Else:\n",
    "    1. $r\\leftarrow r+t$.\n",
    "    2. Output the integer encoded in the $k$ least significant bits of $r$ using a binary code.\n",
    "\n",
    "### Example ($m=7$ and $s=8$)\n",
    "\n",
    "1. [1] $k\\leftarrow \\lceil\\log_2(8)\\rceil=3$.\n",
    "2. [2] $r\\leftarrow 8 \\text{mod} 7 = 1$.\n",
    "3. [3] $t\\leftarrow 2^3-7 = 8-7 = 1$.\n",
    "4. [4] Output $\\leftarrow 8 \\text{div} 7 = \\lfloor 8/7\\rfloor=1$ as an unary code (2 bits). Therefore, output $\\leftarrow 10$.\n",
    "5. [5] $r=1\\le t=1$.\n",
    "6. [6.A] $r\\leftarrow 1+1=2$.\n",
    "7. [6.B] Output $r=2$ using a binary code of $k=3$ bits. Therefore, $c(8)=10010$.\n",
    "\n",
    "### Decoder\n",
    "\n",
    "1. $k\\leftarrow\\lceil\\log_2(m)\\rceil$.\n",
    "2. $t\\leftarrow 2^k-m$.\n",
    "3. Let $s\\leftarrow$ the number of consecutive ones in the input (we stop when we read a $0$).\n",
    "4. Let $x\\leftarrow$ the next $k-1$ bits in the input.\n",
    "5. If $x<t$:\n",
    "    1. $s\\leftarrow s\\times m+x$.\n",
    "6. Else:\n",
    "    1. $x\\leftarrow x\\times 2~+$ next input bit.\n",
    "    2. $s\\leftarrow s\\times m+x-t$.\n",
    "    \n",
    "### Example (decode $10010$ where $m=7$)\n",
    "\n",
    "1. [1] $k\\leftarrow 3$.\n",
    "2. [2] $t\\leftarrow 2^k-m = 2^3-7=1$).\n",
    "3. [3] $s\\leftarrow 1$ because we found only one $1$ in the input.\n",
    "4. [4] $x\\leftarrow \\text{input}_{k-1} = \\text{input}_2 = 01$.\n",
    "5. [5] $x=1\\nless t=1$.\n",
    "6. [6.A] $x\\leftarrow x\\times x\\times 2+\\text{next input bit} = x\\times 1\\times 2+0 = 2$.\n",
    "7. [6.B] $s\\leftarrow s\\times m+x-t = 1\\times 7+2-1=8$.\n",
    "\n",
    "#### Lab\n",
    "TO-DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## C.10 Rice coding\n",
    "\n",
    "### Encoder\n",
    "\n",
    "1. $m\\leftarrow 2^k$.\n",
    "2. Output $\\leftarrow\\lfloor s/m\\rfloor$ using an unary code ($\\lfloor s/m\\rfloor+1$ bits).\n",
    "3. Output $\\leftarrow$ the $k$ least significant bits of $s$ using a binary code.\n",
    "    \n",
    "### Example ($k=1$ and $s=7$)\n",
    "1. [1] $m\\leftarrow 2^k=2^1=2$.\n",
    "2. [2] Output $\\leftarrow \\lfloor s/m\\rfloor=\\lfloor 7/2\\rfloor=3$ using an unary code of 4 bits. Therefore, output $\\leftarrow 1110$.\n",
    "3. Output $\\leftarrow$ the $k=1$ least significant bits of $s=7$\n",
    "  using a unary code ($k+1$ bits). So, output $\\leftarrow 1$. Total\n",
    "  output $c(7)=11101$.\n",
    "\n",
    "### Decoder\n",
    "\n",
    "1. Let $s$ the number of consecutive ones in the input (we stop when we read a 0).\n",
    "2. Let $x$ the next $k$ input bits.\n",
    "3. $s\\leftarrow s\\times 2^k+x$.\n",
    "\n",
    "### Example (decode $11101$ where $k=1$)\n",
    "1. [1] $s\\leftarrow 3$ because we found $3$ consecutive ones in the input.\n",
    "2. [2] $x\\leftarrow$ next input $k=1$ input bits. Therefore $x\\leftarrow 1$.\n",
    "3. [3] $s\\leftarrow s\\times 2^k+x = 3\\times 2^1+1 = 6+1 = 7$.\n",
    "\n",
    "### Lab\n",
    "TO-DO"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
